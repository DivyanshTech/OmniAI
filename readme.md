
Production-grade AI assistant capable of handling business customer support, general queries, and context-aware conversations. Features include:

- RAG (Retrieval Augmented Generation) for accurate responses from knowledge bases
- Conversation memory to track user context
- Real-time logging and analytics
- Integration with multiple LLMs (OpenAI/Grok/Local models)
ğŸ¯ Overview
This is a fully functional, production-ready customer support chatbot that combines modern AI technologies to provide intelligent, context-aware responses. The system uses RAG to retrieve relevant information from a knowledge base and generates natural language responses using state-of-the-art LLMs.
Key Highlights
âœ… RESTful API built with FastAPI
âœ… Interactive UI using Streamlit
âœ… RAG Pipeline with sentence transformers
âœ… Conversation Memory (sliding window)
âœ… Real-time Logging & analytics
âœ… Multi-LLM Support (Groq/OpenAI)
âœ… Vector Search with FAISS
âœ… Deployment-ready (Render/Railway/AWS)

âœ¨ Features
Core Features

ğŸ§  RAG (Retrieval Augmented Generation)

Semantic search over knowledge base
Top-K document retrieval
Context-aware responses

ğŸ’¬ Conversation Memory
Maintains context across messages
Sliding window (last 10 messages)
Session management

ğŸ¯ Multi-Source Knowledge Base
20 comprehensive FAQs
6 detailed policy documents
Categorized information (Account, Billing, Technical, etc.)


ğŸ“Š Logging & Analytics
Every interaction logged
Daily statistics
Processing time tracking
Error monitoring


Technical Features
âš¡ Async API (FastAPI)
ğŸ”’ CORS configured for security
ğŸ” Vector embeddings (384-dimensional)
ğŸ¨ Custom UI with chat bubbles
ğŸ“ˆ Health monitoring endpoints
ğŸ›¡ï¸ Error handling & fallback responses

 Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                       â”‚
â”‚                      (Streamlit Frontend)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP Request
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FASTAPI BACKEND                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Receive User Query                               â”‚   â”‚
â”‚  â”‚  2. Load Conversation History (Memory)               â”‚   â”‚
â”‚  â”‚  3. Retrieve Context (RAG Engine)                    â”‚   â”‚
â”‚  â”‚  4. Build Prompt (Context + History + Query)         â”‚   â”‚
â”‚  â”‚  5. Generate Response (LLM Engine)                   â”‚   â”‚
â”‚  â”‚  6. Save to Memory                                   â”‚   â”‚
â”‚  â”‚  7. Log Interaction (Logger)                         â”‚   â”‚
â”‚  â”‚  8. Return JSON Response                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RAG   â”‚    â”‚   LLM    â”‚    â”‚ Memory  â”‚
    â”‚ Engine â”‚    â”‚  Engine  â”‚    â”‚ Handler â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vector â”‚    â”‚ Groq/GPT â”‚    â”‚ Session â”‚
    â”‚ Store  â”‚    â”‚   API    â”‚    â”‚  Store  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Data Flow
User Query â†’ Memory Retrieval â†’ RAG Context Search â†’ Prompt Build
â†’ LLM Generation â†’ Memory Update â†’ Logging â†’ Response to User

ğŸ› ï¸ Tech Stack
Backend

FastAPI - Modern async web framework
Uvicorn - ASGI server
Pydantic - Data validation

AI/ML

Sentence Transformers - Text embeddings
Groq/OpenAI APIs - LLM inference
Scikit-learn - Cosine similarity
NumPy - Vector operations

Frontend

Streamlit - Interactive web UI
Requests - HTTP client

Storage

JSON - Knowledge base storage
Pickle - Vector store serialization
File-based - Conversation logs


ğŸ“‚ Project Structure
chatbot-project/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI main server
â”‚   â”œâ”€â”€ llm_engine.py       # LLM integration (Groq/OpenAI)
â”‚   â”œâ”€â”€ rag_engine.py       # Vector search & retrieval
â”‚   â”œâ”€â”€ memory.py           # Conversation memory handler
â”‚   â”œâ”€â”€ database.py         # Knowledge base loader
â”‚   â””â”€â”€ logger.py           # Logging & analytics
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ ui.py               # Streamlit interface
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ faqs.json           # 20 FAQ entries
â”‚   â””â”€â”€ policies.json       # 6 policy documents
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vector_store/       # Saved embeddings
â”‚       â””â”€â”€ vectors.pkl     # Pickle file
â”‚
â”œâ”€â”€ logs/                   # Auto-generated logs
â”‚   â”œâ”€â”€ chat_log_YYYY-MM-DD.json
â”‚   â”œâ”€â”€ errors.json
â”‚   â”œâ”€â”€ system.log
â”‚   â””â”€â”€ app.log
â”‚
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment variables template
â”œâ”€â”€ .gitignore             # Git ignore rules
â””â”€â”€ README.md              # This file

ğŸš€ Setup & Installation
Prerequisites

Python 3.8 or higher
pip (Python package manager)
Git
API key from Groq (free) or OpenAI (paid)

Step 1: Clone Repository
bashgit clone https://github.com/yourusername/chatbot-project.git
cd chatbot-project
Step 2: Create Virtual Environment
bash# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate
Step 3: Install Dependencies
bashpip install -r requirements.txt
Step 4: Setup Environment Variables
bash# Copy example file
cp .env.example .env

# Edit .env and add your API key
# For Groq (FREE): Get key from https://console.groq.com
# For OpenAI: Get key from https://platform.openai.com/api-keys
Example .env:
bashGROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx
# or
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxx
Step 5: Initialize Vector Store
bashcd backend
python rag_engine.py
This will:

Load FAQs and policies
Generate embeddings
Save vector store to models/vector_store/


ğŸ’» Usage
Running Locally
Start Backend Server
bashcd backend
uvicorn app:app --reload --host 0.0.0.0 --port 8000
Backend will be available at: http://localhost:8000
Start Frontend UI
Open a new terminal:
bashcd frontend
streamlit run ui.py
Frontend will be available at: http://localhost:8501
Testing API Endpoints
bash# Health check
curl http://localhost:8000/health

# Chat endpoint
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "How do I reset my password?",
    "session_id": "test123",
    "include_context": true,
    "top_k": 3
  }'

# Get statistics
curl http://localhost:8000/stats

ğŸ“šAPI Documentation
Endpoints
GET /
Root endpoint with API information
GET /health
Health check endpoint

Returns service status
Checks RAG, LLM, Memory, Logger

POST /chat
Main chat endpoint
Request Body:
json{
  "message": "Your question here",
  "session_id": "optional-session-id",
  "include_context": true,
  "top_k": 3
}
Response:
json{
  "success": true,
  "response": "Bot response here",
  "session_id": "session-uuid",
  "processing_time": 1.23,
  "context_used": 3,
  "error": null
}
GET /stats
Get system statistics
DELETE /clear_session/{session_id}
Clear conversation history
GET /session/{session_id}
Get session history
Interactive API Docs:

Swagger UI: http://localhost:8000/docs
ReDoc: http://localhost:8000/redoc

ğŸ“¸ Screenshots and Demo video
![Chatbot UI](https://raw.githubusercontent.com/DivyanshTech/OmniAI/main/assets/Chatbot%20Intro.png)
![General Question](https://raw.githubusercontent.com/DivyanshTech/OmniAI/main/assets/General%20question.png)
![How do I Contact Support](https://raw.githubusercontent.com/DivyanshTech/OmniAI/main/assets/How%20do%20i%20Contact%20support.png)
![Tech Question](https://raw.githubusercontent.com/DivyanshTech/OmniAI/main/assets/Tech%20Question.png)
[â–¶ Watch Demo Video](https://raw.githubusercontent.com/DivyanshTech/OmniAI/main/assets/Customer%20Support%2Bgeneral%20purpose%20Chatbot%20-%20Demo.mp4)


